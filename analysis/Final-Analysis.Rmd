---
title: "Final Analysis"
author: "Joshua White"
date: "1 April 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 2)#sets inline code, and all code to only show 2 decimal places.
library(tidyverse)
library(here)
library(car) 
library(ordinal)
library(MASS)
library(brms)
library(skimr)

set.seed(1298)
```

## Load Packages and set seed for reproducibility

```{r load-packages, eval = FALSE}
library(tidyverse)
library(here)
library(car) # for recode function
library(ordinal) # for clmm (cumulative link mixed model) function for frequentist mixed models
library(MASS) # for Polr fucntion(proportional odds logistic regression) when no random effects (clmm doesn't work here)
library(brms)
library(skimr)

set.seed(1298)
```

##Import Data

```{r import-data, warning = FALSE}
d <- read_csv("parsedrawresults.csv", col_types = "icffffnnfnfc")
d <- d[,2:12]
glimpse(d)
```

##Data Wrangling
####Recode VignetteAnswer variable, create new variable, and reorder data  

```{r data-wrangling}
d$vignetteAnswer[d$answerOrder == "A"] <- recode(d$vignetteAnswer[d$answerOrder == "A"], "1 = 3;
                                                                                          2 = 2;
                                                                                          3 = 1;
                                                                                          4 = 0;
                                                                                          5 = -1;
                                                                                          6 = -2;
                                                                                          7 = -3")

d$vignetteAnswer[d$answerOrder == "B"] <- recode(d$vignetteAnswer[d$answerOrder == "B"], "1 = -3;
                                                                                          2 = -2;
                                                                                          3 = -1;
                                                                                          4 = 0;
                                                                                          5 = 1;
                                                                                          6 = 2;
                                                                                          7 = 3")

###create new variable for vignette type + number -- vignette###
# order levels of factor for easy graphing
order <- c(sprintf("G%d", seq(1:12)), sprintf("L%d", seq(1:12))) 
d$vignette <- paste0(d$vignetteType, d$vignetteNumber) %>% factor(levels = order)

#reorder data frame
d <- d[, c(1:5, 12, 6:11)]
```

## Instruction Checks 

```{r instruction-checks}
# first remove amy's testing data point
amy.testing <- grep("amy", d$postquestion2)
d_amyremoved <- d[-amy.testing,]

#check whether observations fail instruction checks
instr_fail <- d_amyremoved$postquestion1 == "incorrect" | d_amyremoved$dummyVignetteAnswer <= 4

#remove data from main dataset and save for possible later analysis (was a particular question more confusing etc?)
d_removed <- d_amyremoved[instr_fail,]
d_pass <- d_amyremoved[!instr_fail,]

d_removed
```
  
## Quickly Inspect Final Data 

```{r inspect-data}
skim(d_pass)
```

## Graph responses

```{r graph}
#make plot
ggplot(d_pass, aes(x = vignette, y = vignetteAnswer, fill = vignetteType)) + 
  geom_jitter(width = 0.25, height = 0.10) +
  stat_summary(fun.y=mean, geom="bar", alpha = 0.8) +
  scale_y_continuous(breaks = -3:3, minor_breaks = NULL, limits = (c(-3.10,3.10)))

```

## Data analysis: Fit ordinal logistic regression models

First, make vignetteAnswer an ordered factor
```{r order-vignetteAnswer}
d_pass$vignetteAnswer.ordfac <- ordered(d_pass$vignetteAnswer, levels = c("-3", "-2", "-1", "0", "1", "2", "3"))

```

###Frequentist

```{r frequentist-analysis}
olm.freq.nopred <-  polr(vignetteAnswer.ordfac ~ 1, data = d_pass, method = "logistic", Hess = TRUE) 
olm.freq.GLpred <-  polr(vignetteAnswer.ordfac ~ vignetteType, data = d_pass, method = "logistic", Hess = TRUE) 
olm.freq.ordpred <- polr(vignetteAnswer.ordfac ~ answerOrder, data = d_pass, method = "logistic", Hess = TRUE) 
olmm.freq <-  clmm(vignetteAnswer.ordfac ~ vignetteType + (1|vignette), data = d_pass, link = "logit", nAGQ = 10) #values between 5 and 10 generally provide accurate maximum likelihood estimates -- documentation

summary(olm.freq.nopred)
summary(olm.freq.GLpred)
summary(olm.freq.ordpred)
summary(olmm.freq)
```

####AIC values
The AIC values for our four different frequentist models are:

* `r AIC(olm.freq.nopred)` for the model with no predictors 
* `r AIC(olm.freq.GLpred)` for the model with Gain or Loss as a predictor
* `r AIC(olm.freq.ordpred)` for the model with answer order as a predictor
* `r AIC(olmm.freq)` for the mixed model with Gain or Loss and a random effect for vignette as predictors

Therefore, the most effective model is the mixed model, showing that there particpant ambiguity aversion is predicted both by whether the vignette was about a gain or a loss, and a random effect for vignette. The model taking into account answer order as a predictor had the worst Akaike Information Criterion, providing evidence that answer order did not effect participant responses. 

### Bayesian

```{r bayesian-analysis, warning = FALSE}
olm.bayes.nopred <-  brm(vignetteAnswer.ordfac ~ 1, data = d_pass, family = cumulative(link = "logit"), 
                         refresh = 0, open_progress = FALSE)  #to suppress progress printing to screen for .Rmd
olm.bayes.GLpred <-  brm(vignetteAnswer.ordfac ~ vignetteType, data = d_pass, family = cumulative(link = "logit"), 
                         refresh = 0, open_progress = FALSE) 
olm.bayes.ordpred <- brm(vignetteAnswer.ordfac ~ answerOrder, data = d_pass, family = cumulative(link = "logit"), 
                         refresh = 0, open_progress = FALSE) 
olmm.bayes <- brm(vignetteAnswer.ordfac ~ vignetteType + (1|vignette), data = d_pass, family = cumulative(link = "logit"), 
                  refresh = 0, open_progress = FALSE) 

summary(olm.bayes.nopred)
summary(olm.bayes.GLpred)
summary(olm.bayes.ordpred)
summary(olmm.bayes)
```

####WAIC values
The WAIC values for our four different Bayesian models are (I was not able to easily extract the AIC from these models):

* `r WAIC(olm.bayes.nopred)$estimates[3,1]` for the model with no predictors 
* `r WAIC(olm.bayes.GLpred)$estimates[3,1]` for the model with Gain or Loss as a predictor
* `r WAIC(olm.bayes.ordpred)$estimates[3,1]` for the model with answer order as a predictor
* `r WAIC(olmm.bayes)$estimates[3,1]` for the mixed model with Gain or Loss and a random effect for vignette as predictors

Therefore, the most effective model is the mixed model, showing that there particpant ambiguity aversion is predicted both by whether the vignette was about a gain or a loss, and a random effect for vignette. The model taking into account answer order as a predictor had the worst WAIC, providing evidence that answer order did not effect participant responses.

## Urn Analysis
### Plot 
```{r urn-plot}
##filter dataset to contain only urn vignettes
d_pass_urns <- d_pass %>% filter(vignette == "G10" | vignette == "L7")

##plot
ggplot(d_pass_urns, aes(x = vignette, y = vignetteAnswer, fill = vignetteType)) + 
  geom_jitter(width = 0.25, height = 0.10) +
  stat_summary(fun.y=mean, geom="bar", alpha = 0.8) +
  scale_y_continuous(breaks = -3:3, minor_breaks = NULL, limits = (c(-3.10,3.10))) +
  ggtitle("Urn Vignette Responses")
```

###Statistical analyses

####Frequentist
```{r frequentist-analysis-urns}
olm.freq.nopred.urns <-  polr(vignetteAnswer.ordfac ~ 1, data = d_pass_urns,
                              method = "logistic", Hess = TRUE) 
olm.freq.GLpred.urns <-  polr(vignetteAnswer.ordfac ~ vignetteType, data = d_pass_urns,
                              method = "logistic", Hess = TRUE) 

summary(olm.freq.nopred.urns)
summary(olm.freq.GLpred.urns)
```

####AIC values
The AIC values for our two different frequentist models are:

* `r AIC(olm.freq.nopred.urns)` for the model with no predictors 
* `r AIC(olm.freq.GLpred.urns)` for the model with Gain or Loss as a predictor

These values illustrate that the existing effect is replicated in our findings. Firstly, ambiguity aversion exists in both gains and loss variants of the classic urn task. Secondly, the degree of ambiguity aversion is affected by whether we are discussing domains of loss or gain --- ambiguity aversion is stronger for gains than for losses. 

#### Bayesian

```{r bayesian-analysis-urns, warning = FALSE}
olm.bayes.nopred.urns <- brm(vignetteAnswer.ordfac ~ 1, data = d_pass_urns, family = cumulative(link = "logit"), 
                         refresh = 0, open_progress = FALSE)  #to suppress progress printing to screen for .Rmd
olm.bayes.GLpred.urns <- brm(vignetteAnswer.ordfac ~ vignetteType, data = d_pass_urns, family = cumulative(link = "logit"), 
                         refresh = 0, open_progress = FALSE) 

summary(olm.bayes.nopred.urns)
summary(olm.bayes.GLpred.urns)
```

####WAIC values
The WAIC values for our four different Bayesian models are (I was not able to easily extract the AIC from these models):

* `r WAIC(olm.bayes.nopred.urns)$estimates[3,1]` for the model with no predictors 
* `r WAIC(olm.bayes.GLpred.urns)$estimates[3,1]` for the model with Gain or Loss as a predictor

These values illustrate that the existing effect is replicated in our findings. Firstly, ambiguity aversion exists in both gains and loss variants of the classic urn task. Secondly, the degree of ambiguity aversion is affected by whether we are discussing domains of loss or gain --- ambiguity aversion is stronger for gains than for losses. 